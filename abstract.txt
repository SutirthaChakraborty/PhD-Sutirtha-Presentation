\chapter*{Abstract}
\pagestyle{plain}

The emerging field of “Robotic Musicianship" focuses on developing machine intelligence, in terms of algorithms and cognitive models, to capture musical perception, composition, and performance, and to transplant these skills into a robot that can then reproduce them in any context. In such multi-ensemble technological settings, it must be assumed that humans will not play rigidly; rather they will move and express with the ‘feel' of the music, wherein the roles of ‘leader' and ‘follower' within the troupe could often change in a fluid manner.  Hence, for machines to participate and create cooperative musical performances, where synchronization and adaptation plays a vital role, they need to operate at a higher cognitive level (i.e., decode rhythm and pitch information from sound signals and non-verbal gestures). Therefore, in this thesis, we explore how real-time collaborations between humans and machines can be improved by integrating models from the technologies of Oscillator Synchronization and of Machine Learning, thus driving closer towards a vision of a human-robot symphonic orchestra.
To explicitly capture the above, we develop an approach based on the joint strategy of (i) Mapping – responsible for ensuring control and sensing of the components of musical instruments along with the parameters in sound synthesis, and (ii) Modeling – focuses on capturing the overall representation of the musical process.   We consider each musician (irrespective of human or machine) as a separate oscillator, wherein mathematical models for oscillator coupling, for example the well-known Kuramoto algorithm, can be used for establishing and maintaining synchronization. This takes into account the musical parameters like beat and tempo and would form the mapping phase. The modelling stage employs deep learning predictive models like LSTM-based Neural Network architectures to capture long-distance patterns and relations in the music sequence to learn the overall musical feature space comprising beat, onset, sustain, decay and pitch. This enables the robot system to anticipate future state changes in the sequence of music generation and act pre-emptively to respond to transitions. The results of the thesis will show that integrating oscillator-based synchronization with deep learning models enables robust, real-time adaptation to dynamic tempo shifts and fluid leader–follower transitions in musical ensembles. The multimodal framework—leveraging both audio and visual cues—significantly enhances synchronization accuracy and expressive performance, paving the way for natural human–robot musical collaboration. To conclude, it is intended that this thesis will foster new insights and research in the direction of human-robot ensemble-based music creation and expressive performance, and will inspire better real-time synchronized human-computer collaborative interfaces".