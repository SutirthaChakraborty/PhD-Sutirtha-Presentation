<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Introduction - PhD Thesis</title>
    <link rel="stylesheet" href="styles.css">
    
    <!-- MathJax Configuration -->
    <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams'
        },
        svg: {
            fontCache: 'global',
            displayAlign: 'center',
            displayIndent: '0em'
        },
        startup: {
            ready: () => {
                MathJax.startup.defaultReady();
                console.log('MathJax loaded for Chapter 1');
            }
        }
    };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="progress-container">
        <div class="progress-bar"></div>
    </div>

    <header class="header">
        <div class="container">
            <h1>Chapter 1: Introduction</h1>
            <h2>The Evolution of Automated Musical Systems and Human-Robot Synchronization</h2>
            <div class="author">Sutirtha Chakraborty</div>
            <div class="university">Maynooth University</div>
        </div>
    </header>

    <nav class="nav">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">PhD Thesis</a>
            <div class="nav-toggle">
                <span></span>
                <span></span>
                <span></span>
            </div>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="chapter1.html">Ch 1: Introduction</a></li>
                <li><a href="chapter2.html">Ch 2: Literature</a></li>
                <li><a href="chapter3.html">Ch 3: Framework</a></li>
                <li><a href="chapter4.html">Ch 4: LeaderSTeM</a></li>
                <li><a href="chapter5.html">Ch 5: Visual Cues</a></li>
                <li><a href="chapter6.html">Ch 6: Virtual</a></li>
                <li><a href="chapter7.html">Ch 7: Nature</a></li>
                <li><a href="chapter8.html">Ch 8: Multimodal</a></li>
            </ul>
        </div>
    </nav>

    <!-- Breadcrumb Navigation -->
    <div class="container">
        <nav class="breadcrumb">
            <a href="index.html">Home</a>
            <span class="separator">‚Ä∫</span>
            <span class="current">Chapter 1: Introduction</span>
        </nav>
    </div>

    <div class="container">
        <div class="card" id="introduction">
            <h2>Introduction</h2>
            <p>Music has been an integral part of human civilization, profoundly expressing creativity, emotion, and culture. From the rhythmic beats of ancient rituals to the complex harmonies of modern-day performances, music has evolved alongside humanity, reflecting artistic trends and social and technological changes.</p>

            <div class="image-container">
                <img src="images/introduction/cyborg_intro.jpg" alt="Cyborg Musical Interaction" style="max-height: 400px;">
                <div class="image-caption">The vision of human-robot musical collaboration represents the convergence of technology and artistic expression</div>
            </div>

            <p>At the heart of both music and technology lies the concept of <span class="tooltip">synchronization<span class="tooltiptext">The coordination of simultaneous processes or events to operate in unison</span></span>. This concept is not limited to music; it is a fundamental principle observed in various aspects of life:</p>

            <ul>
                <li><strong>Walking Together:</strong> People unconsciously adjust their pace to match each other's steps</li>
                <li><strong>Conversation:</strong> Speakers synchronize their speech patterns and rhythms for fluid communication</li>
                <li><strong>Computer Networks:</strong> Multiple machines coordinate to work together efficiently</li>
                <li><strong>Traffic Systems:</strong> Traffic lights are synchronized to optimize flow and safety</li>
            </ul>

            <h3>Computer Assisted Music Making (CAMM)</h3>
            <p><span class="tooltip">CAMM<span class="tooltiptext">Computer Assisted Music Making - the integration of technology in music creation, helping musicians in both composition and performance</span></span> represents the integration of technology in music creation, helping musicians in both composition and performance. CAMM can be divided into two overlapping categories:</p>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                <div style="background: #f0f9ff; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #0ea5e9;">
                    <h4>üéº Computer Assisted Music Composition (CAMC)</h4>
                    <p>Tools and systems designed to support the composition process, utilizing <span class="tooltip">algorithms<span class="tooltiptext">A set of rules or instructions given to a computer to help it solve problems or complete tasks</span></span> and artificial intelligence to suggest or refine compositions.</p>
                </div>
                
                <div style="background: #f0fdf4; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #22c55e;">
                    <h4>üé™ Computer Assisted Music Performance (CAMP)</h4>
                    <p>Focuses on <span class="tooltip">real-time<span class="tooltiptext">Processing or responding to data immediately as it is received, without delay</span></span> enhancements to live or recorded performance, including interactive systems that respond to a performer's movements.</p>
                </div>
            </div>

            <div class="image-container">
                <img src="images/introduction/camm_venn_diagram.png" alt="CAMM Venn Diagram">
                <div class="image-caption">Venn Diagram of Computer Assisted Music Making (CAMM), illustrating the relationship between CAMC and CAMP</div>
            </div>

            <h3>Key Musical Terms</h3>
            <p>To fully understand the importance of synchronization in music, several key musical terms are essential:</p>

            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem; margin: 2rem 0;">
                <div class="term-card" style="background: #fef3c7; padding: 1rem; border-radius: 8px;">
                    <h4><span class="tooltip">Beat<span class="tooltiptext">The basic unit of time in music, providing the pulse that musicians follow</span></span></h4>
                    <p>The basic unit of time in music, providing the pulse that listeners often clap or tap along to.</p>
                </div>
                
                <div class="term-card" style="background: #dbeafe; padding: 1rem; border-radius: 8px;">
                    <h4><span class="tooltip">Tempo<span class="tooltiptext">The speed or pace of music, usually measured in beats per minute (BPM)</span></span></h4>
                    <p>The speed at which music is played, usually measured in beats per minute (BPM).</p>
                </div>
                
                <div class="term-card" style="background: #fce7f3; padding: 1rem; border-radius: 8px;">
                    <h4><span class="tooltip">Rubato<span class="tooltiptext">A musical technique where the performer slightly speeds up or slows down for expressive effect</span></span></h4>
                    <p>A technique where performers subtly vary tempo for expressive purposes.</p>
                </div>
                
                <div class="term-card" style="background: #dcfce7; padding: 1rem; border-radius: 8px;">
                    <h4>Rhythm</h4>
                    <p>The pattern of sounds and silences in music, providing the framework for movement and timing.</p>
                </div>
                
                <div class="term-card" style="background: #f3e8ff; padding: 1rem; border-radius: 8px;">
                    <h4>Dynamics</h4>
                    <p>Volume levels in music, ranging from soft (piano) to loud (forte), adding emotional depth.</p>
                </div>
                
                <div class="term-card" style="background: #fef2f2; padding: 1rem; border-radius: 8px;">
                    <h4>Polyrhythm</h4>
                    <p>Using contrasting rhythms that require synchronization of complex structures.</p>
                </div>
            </div>
        </div>

        <div class="card" id="evolution">
            <h2>The Evolution of Automated Musical Systems</h2>
            
            <h3>Early Mechanization and the Birth of Automated Music</h3>
            <p>The concept of automated music dates back to ancient times. One of the earliest examples is the <span class="highlight">water organ (hydraulis)</span> from ancient Greece, invented in the 3rd century BCE.</p>

            <div class="image-container">
                <img src="images/introduction/Hydraulis_001.jpg" alt="Ancient Hydraulis">
                <div class="image-caption">Hydraulis - Ancient Greek water-powered organ, one of the earliest examples of automated musical instruments (3rd century BCE)</div>
            </div>

            <h3>The Rise of Mechanical Instruments: 17th to 19th Centuries</h3>
            <p>The 17th and 18th centuries marked significant advancements in mechanized music, including:</p>
            <ul>
                <li><strong>Musical clocks (carillon clocks)</strong> - Played pre-programmed music on bells and chimes</li>
                <li><strong>Mechanical flute player by Jacques de Vaucanson</strong> - Mimicked human-like techniques</li>
                <li><strong>Player piano (pianola)</strong> - Used perforated paper rolls to encode music</li>
            </ul>

            <div class="image-container">
                <img src="images/introduction/Floutiste Theroude.png" alt="Floutiste Automaton">
                <div class="image-caption">"Floutiste" - Life-size Flute Player Automaton by A. Theroude, Paris, France</div>
            </div>

            <h3>Electromechanical and Early Electronic Systems: 20th Century</h3>
            <p>The 20th century brought revolutionary changes with the introduction of electronic instruments:</p>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                <div>
                    <div class="image-container">
                        <img src="images/introduction/theremin.jpg" alt="Theremin Player">
                        <div class="image-caption">Alexandra Stepanoff playing a theremin in 1930</div>
                    </div>
                </div>
                <div>
                    <div class="image-container">
                        <img src="images/introduction/scottworks_electronium.jpg" alt="Electronium">
                        <div class="image-caption">Raymond Scott with the Electronium - pioneering sequencer and automatic music composition machine</div>
                    </div>
                </div>
            </div>

            <h3>The Digital Revolution: Late 20th Century</h3>
            <p>The late 20th century brought the digital revolution, transforming music automation through:</p>
            <ul>
                <li><strong><span class="tooltip">MIDI<span class="tooltiptext">Musical Instrument Digital Interface - a technical standard that describes a communications protocol for electronic musical instruments</span></span> Protocol (1983)</strong> - Enabled communication between electronic instruments and computers</li>
                <li><strong>Sequencer Software</strong> - Programs like Cubase revolutionized home and professional studios</li>
                <li><strong>Algorithmic Composition</strong> - Composers like Iannis Xenakis experimented with computer-generated music</li>
                <li><strong>Early Robot Musicians</strong> - WABOT-2 could read musical scores and play keyboard</li>
            </ul>

            <div class="image-container">
                <img src="images/introduction/wabot2.jpg" alt="WABOT-2">
                <div class="image-caption">WABOT-2 - One of the first humanoid robots capable of reading musical scores and playing a keyboard</div>
            </div>

            <h3>Modern Robotics and AI in Music: 21st Century</h3>
            <p>The 21st century has witnessed the emergence of sophisticated robotic musicians and AI integration:</p>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                <div>
                    <div class="image-container">
                        <img src="images/introduction/shimon.jpg" alt="Shimon Robot">
                        <div class="image-caption">Shimon - Georgia Tech's marimba-playing robot capable of improvisation</div>
                    </div>
                </div>
                <div>
                    <div class="image-container">
                        <img src="images/introduction/Z-Machines.jpg" alt="Z-Machines">
                        <div class="image-caption">Z-Machines - Three-piece robot band bringing new meaning to electronic music</div>
                    </div>
                </div>
            </div>

            <div class="image-container">
                <img src="images/introduction/robot Musician.jpeg" alt="Robot Conductor">
                <div class="image-caption">A robot maestro conducting an orchestra at the Sharjah Performing Arts Academy (2020)</div>
            </div>

            <h3>Machine Learning Techniques for Synchronization</h3>
            <p>Modern synchronization systems leverage advanced <span class="tooltip">machine learning<span class="tooltiptext">A type of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed</span></span> techniques:</p>

            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 2rem 0;">
                <div style="background: #f0f9ff; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #0ea5e9;">
                    <h4><span class="tooltip">CNNs<span class="tooltiptext">Convolutional Neural Network - a deep learning algorithm particularly effective for analyzing visual imagery</span></span></h4>
                    <p>Used for gesture recognition and facial expression analysis in visual synchronization.</p>
                </div>
                
                <div style="background: #f0fdf4; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #22c55e;">
                    <h4><span class="tooltip">LSTMs<span class="tooltiptext">Long Short-Term Memory - a type of recurrent neural network capable of learning long-term dependencies</span></span></h4>
                    <p>Handle sequential data and predict timing changes in musical performances.</p>
                </div>
                
                <div style="background: #fef3c7; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #f59e0b;">
                    <h4><span class="tooltip">Deep Learning<span class="tooltiptext">A subset of machine learning using neural networks with multiple layers to model and understand complex patterns</span></span></h4>
                    <p>Enables integration of <span class="tooltip">multimodal<span class="tooltiptext">Using multiple modes or methods of input/output, such as combining audio, visual, and gestural data</span></span> data sources for comprehensive synchronization.</p>
                </div>
            </div>

            <table>
                <thead>
                    <tr>
                        <th>Era</th>
                        <th>Key Developments</th>
                        <th>Examples</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Early Mechanization</td>
                        <td>Rudimentary machines producing autonomous sound</td>
                        <td>Water organ (hydraulis)</td>
                    </tr>
                    <tr>
                        <td>17th-19th Centuries</td>
                        <td>More sophisticated mechanical instruments</td>
                        <td>Musical clocks, barrel organs, player pianos</td>
                    </tr>
                    <tr>
                        <td>Early 20th Century</td>
                        <td>Introduction of electromechanical instruments</td>
                        <td>Telharmonium, Hammond organ, Theremin</td>
                    </tr>
                    <tr>
                        <td>Late 20th Century</td>
                        <td>Digital revolution, MIDI, algorithmic composition</td>
                        <td>Digital synthesizers, computer-generated music</td>
                    </tr>
                    <tr>
                        <td>21st Century</td>
                        <td>AI and robotics in music</td>
                        <td>Robotic musicians, AI composition systems</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="card" id="synchronization">
            <h2>Synchronization in Human-Robot Musical Interaction</h2>
            
            <h3>The Central Challenge</h3>
            <p>As automated musical systems evolve from simple mechanization to sophisticated robotic performers, <span class="highlight">synchronization</span> emerges as the central challenge. Unlike systems that merely play back pre-recorded music, human-robot musical interaction demands sophisticated synchronization that encompasses both technical precision and musical expression.</p>

            <h3>Defining Synchronization in Human-Robot Musical Ensembles</h3>
            <p>Synchronization in musical performance is the process of aligning timing and rhythmic elements among multiple performers to produce a cohesive outcome. In human-robot ensembles, this extends beyond mechanical timekeeping.</p>

            <h4>The Dual Nature of Synchronization</h4>
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin: 2rem 0;">
                <div style="background: #fef2f2; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #ef4444;">
                    <h4>‚öôÔ∏è Technical Precision</h4>
                    <p>The robot's capacity to execute musical events with accurate timing, pitch, and dynamics. Many robotic musicians excel at maintaining consistent tempo.</p>
                </div>
                
                <div style="background: #f0fdf4; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #22c55e;">
                    <h4>üé® Musical Expression</h4>
                    <p>Emerges from nuanced variations in timing, dynamics, and articulation. Humans naturally introduce micro-timing shifts and adjust their playing.</p>
                </div>
            </div>

            <h3>Challenges and Approaches</h3>
            <h4>Temporal Precision and Real-Time Adaptation</h4>
            <p>A central challenge is aligning musical events with the micro-timing fluctuations characteristic of human performances. <span class="tooltip">Real-time<span class="tooltiptext">Processing or responding to data immediately as it is received, without delay</span></span> tempo tracking enables robots to continuously estimate and adjust to the ensemble's evolving speed.</p>

            <h4>Dynamic Interaction and Predictive Modelling</h4>
            <p>Synchronization is inherently interactive. Robotic performers must anticipate fluctuations using predictive <span class="tooltip">algorithms<span class="tooltiptext">A set of rules or instructions given to a computer to help it solve problems or complete tasks</span></span> trained on historical performance data.</p>

            <h4>Minimizing Latency</h4>
            <p>Latency‚Äîthe delay between sensing musical input and executing a response‚Äîmust be minimized for robots to match human response times. Any significant delay can disrupt ensemble cohesion.</p>

            <h3>Single-Mode vs. Multimodal Synchronization</h3>
            <div style="text-align: center; margin: 2rem 0;">
                <svg width="600" height="300" viewBox="0 0 600 300" xmlns="http://www.w3.org/2000/svg">
                    <!-- Audio Analysis -->
                    <rect x="50" y="50" width="120" height="60" fill="#dbeafe" stroke="#3b82f6" stroke-width="2" rx="8"/>
                    <text x="110" y="85" text-anchor="middle" font-size="14" font-weight="bold">Audio Analysis</text>
                    
                    <!-- Visual Analysis -->
                    <rect x="240" y="50" width="120" height="60" fill="#dcfce7" stroke="#22c55e" stroke-width="2" rx="8"/>
                    <text x="300" y="85" text-anchor="middle" font-size="14" font-weight="bold">Visual Analysis</text>
                    
                    <!-- Gestural Analysis -->
                    <rect x="430" y="50" width="120" height="60" fill="#fef3c7" stroke="#f59e0b" stroke-width="2" rx="8"/>
                    <text x="490" y="85" text-anchor="middle" font-size="14" font-weight="bold">Gestural Analysis</text>
                    
                    <!-- Synchronization Engine -->
                    <rect x="200" y="160" width="200" height="60" fill="#f3e8ff" stroke="#8b5cf6" stroke-width="2" rx="8"/>
                    <text x="300" y="195" text-anchor="middle" font-size="16" font-weight="bold">Synchronization Engine</text>
                    
                    <!-- Robotic Performance -->
                    <rect x="225" y="250" width="150" height="40" fill="#fce7f3" stroke="#ec4899" stroke-width="2" rx="8"/>
                    <text x="300" y="275" text-anchor="middle" font-size="14" font-weight="bold">Robotic Performance</text>
                    
                    <!-- Arrows -->
                    <line x1="110" y1="110" x2="250" y2="160" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <line x1="300" y1="110" x2="300" y2="160" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <line x1="490" y1="110" x2="350" y2="160" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <line x1="300" y1="220" x2="300" y2="250" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead)"/>
                    
                    <!-- Arrow marker -->
                    <defs>
                        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#374151"/>
                        </marker>
                    </defs>
                </svg>
            </div>
            <div class="image-caption">Conceptual flow of multimodal synchronization combining audio, visual, and gestural inputs</div>

            <h4>Multimodal Sensor Fusion</h4>
            <p>Sensor fusion integrates data streams from microphones, cameras, and motion sensors, enabling robots to track tempo, conductor gestures, and performers' movements simultaneously. However, this poses challenges:</p>
            <ul>
                <li><strong>Data Rate Mismatch:</strong> Audio signals have higher sampling frequencies than visual feeds</li>
                <li><strong>Latency and Noise:</strong> Each sensor introduces its own delay and noise profile</li>
                <li><strong>Dynamic Weighting:</strong> Adaptive <span class="tooltip">algorithms<span class="tooltiptext">A set of rules or instructions given to a computer to help it solve problems or complete tasks</span></span> must assign weights based on context</li>
            </ul>
        </div>

        <div class="card" id="thesis-goals">
            <h2>Thesis Goals</h2>
            
            <p>This thesis develops an integrated framework to synchronize human musicians and robots, enabling dynamic, expressive, and adaptive musical interactions.</p>

            <div style="text-align: center; margin: 2rem 0;">
                <svg width="400" height="500" viewBox="0 0 400 500" xmlns="http://www.w3.org/2000/svg">
                    <!-- Flowchart boxes -->
                    <rect x="50" y="30" width="300" height="50" fill="#fef2f2" stroke="#ef4444" stroke-width="2" rx="8"/>
                    <text x="200" y="60" text-anchor="middle" font-size="14" font-weight="bold">Reviewing State of the Art</text>
                    
                    <rect x="50" y="110" width="300" height="50" fill="#fff7ed" stroke="#f97316" stroke-width="2" rx="8"/>
                    <text x="200" y="135" text-anchor="middle" font-size="12" font-weight="bold">Design Multimodal</text>
                    <text x="200" y="150" text-anchor="middle" font-size="12" font-weight="bold">Synchronization Framework</text>
                    
                    <rect x="50" y="190" width="300" height="50" fill="#fefce8" stroke="#eab308" stroke-width="2" rx="8"/>
                    <text x="200" y="215" text-anchor="middle" font-size="12" font-weight="bold">Advance Predictive Modeling</text>
                    <text x="200" y="230" text-anchor="middle" font-size="12" font-weight="bold">for Expressive Adaptation</text>
                    
                    <rect x="50" y="270" width="300" height="50" fill="dcfce7" stroke="#22c55e" stroke-width="2" rx="8"/>
                    <text x="200" y="295" text-anchor="middle" font-size="12" font-weight="bold">Implement Continuous Learning</text>
                    <text x="200" y="310" text-anchor="middle" font-size="12" font-weight="bold">and Feedback Integration</text>
                    
                    <rect x="50" y="350" width="300" height="50" fill="#dbeafe" stroke="#3b82f6" stroke-width="2" rx="8"/>
                    <text x="200" y="375" text-anchor="middle" font-size="12" font-weight="bold">Evaluate Scalability</text>
                    <text x="200" y="390" text-anchor="middle" font-size="12" font-weight="bold">Across Contexts</text>
                    
                    <rect x="50" y="430" width="300" height="50" fill="#fef2f2" stroke="#ef4444" stroke-width="2" rx="8"/>
                    <text x="200" y="460" text-anchor="middle" font-size="14" font-weight="bold">Robotic Implementation</text>
                    
                    <!-- Arrows -->
                    <line x1="200" y1="80" x2="200" y2="110" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead2)"/>
                    <line x1="200" y1="160" x2="200" y2="190" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead2)"/>
                    <line x1="200" y1="240" x2="200" y2="270" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead2)"/>
                    <line x1="200" y1="320" x2="200" y2="350" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead2)"/>
                    <line x1="200" y1="400" x2="200" y2="430" stroke="#374151" stroke-width="2" marker-end="url(#arrowhead2)"/>
                    
                    <defs>
                        <marker id="arrowhead2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                            <polygon points="0 0, 10 3.5, 0 7" fill="#374151"/>
                        </marker>
                    </defs>
                </svg>
            </div>

            <h3>Key Objectives</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; margin: 2rem 0;">
                <div style="background: #f0f9ff; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #0ea5e9;">
                    <h4>üîÑ Multimodal Framework</h4>
                    <p>Develop a system that integrates audio, visual, and gestural inputs for musical synchronization using <span class="tooltip">machine learning<span class="tooltiptext">A type of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed</span></span> techniques.</p>
                </div>
                
                <div style="background: #f0fdf4; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #22c55e;">
                    <h4>üîÆ Predictive Modeling</h4>
                    <p>Create and refine <span class="tooltip">deep learning<span class="tooltiptext">A subset of machine learning using neural networks with multiple layers to model and understand complex patterns</span></span> models to predict musical parameters such as tempo, dynamics, and expressive timing.</p>
                </div>
                
                <div style="background: #fef3c7; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #f59e0b;">
                    <h4>üìö Continuous Learning</h4>
                    <p>Incorporate continuous learning mechanisms using <span class="tooltip">real-time<span class="tooltiptext">Processing or responding to data immediately as it is received, without delay</span></span> feedback from human musicians.</p>
                </div>
                
                <div style="background: #f3e8ff; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #8b5cf6;">
                    <h4>üé≠ Scalability Testing</h4>
                    <p>Evaluate the system across diverse musical contexts‚Äîdifferent ensemble sizes, genres, and acoustic environments.</p>
                </div>
            </div>

            <h3>Thesis Structure Overview</h3>
            <table>
                <thead>
                    <tr>
                        <th>Chapter</th>
                        <th>Focus</th>
                        <th>Key Contributions</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td>Introduction and Overview</td>
                        <td>Context for research gaps and objectives</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>Literature Review</td>
                        <td>Identifies gaps in multimodal integration</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>Cyborg Philharmonic Framework</td>
                        <td>Novel multimodal synchronization framework</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>LeaderSTeM</td>
                        <td>LSTM-based leader identification</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>Visual Cues</td>
                        <td>Real-time expressive synchronization</td>
                    </tr>
                    <tr>
                        <td>6</td>
                        <td>Multimodal Synchronization</td>
                        <td>Experimental evaluation across contexts</td>
                    </tr>
                    <tr>
                        <td>7</td>
                        <td>Implementation</td>
                        <td>Continuous learning with user feedback</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>Conclusion</td>
                        <td>Future research directions</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="chapter-nav">
            <a href="index.html" class="btn">‚Üê Back to Home</a>
            <a href="chapter2.html" class="btn">Chapter 2: Literature Review ‚Üí</a>
        </div>
    </div>

    <button class="back-to-top">‚Üë</button>

    <script src="main.js"></script>
    <script>
        // Chapter 1 specific functionality
        document.addEventListener('DOMContentLoaded', function() {
            // Add hover effects to term cards
            document.querySelectorAll('.term-card').forEach(card => {
                card.addEventListener('mouseenter', function() {
                    this.style.transform = 'scale(1.02)';
                    this.style.transition = 'all 0.3s ease';
                });
                
                card.addEventListener('mouseleave', function() {
                    this.style.transform = 'scale(1)';
                });
            });

            // Timeline animation for evolution section
            const observerOptions = {
                threshold: 0.1,
                rootMargin: '0px 0px -100px 0px'
            };

            const observer = new IntersectionObserver(function(entries) {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.style.opacity = '1';
                        entry.target.style.transform = 'translateX(0)';
                    }
                });
            }, observerOptions);

            // Apply animation to all cards in evolution section
            document.querySelectorAll('#evolution .card, #evolution .image-container').forEach(el => {
                el.style.opacity = '0';
                el.style.transform = 'translateX(-20px)';
                el.style.transition = 'all 0.6s ease';
                observer.observe(el);
            });
        });
    </script>
</body>
</html>